"""Download pipeline - download media files.

ðŸš€ FAST AS FUCK EDITION ðŸš€

Architecture (Simple is Fast):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. LOAD PHASE (Thread Pool)                                     â”‚
â”‚     - Load DB into DataFrame (pandas SQL query)                  â”‚
â”‚     - Scan filesystem into set (os.scandir)                      â”‚
â”‚     - Compare in memory â†’ get download list                      â”‚
â”‚     Duration: ~30 seconds for 6M items                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  2. DOWNLOAD PHASE (Async I/O)                                   â”‚
â”‚     - Feed download list to async workers                        â”‚
â”‚     - N concurrent downloads with aiohttp                        â”‚
â”‚     - Progress bar updates in real-time                          â”‚
â”‚     Duration: depends on network, ~50 files/sec typical          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Key optimizations:
- No item-by-item iteration (vectorized pandas ops)
- No complex queue patterns (simple list â†’ workers)
- No blocking in event loop (all heavy work in executor)
- Minimal object creation (work with raw data)
- Connection pooling with keep-alive
- Fibonacci backoff for rate limits (be nice to server)
"""

from __future__ import annotations

import asyncio
import logging
import os
import sqlite3
import time
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from pathlib import Path
from typing import Optional

import aiohttp
import pandas as pd
from rich.progress import (
    Progress,
    SpinnerColumn,
    BarColumn,
    TextColumn,
    TimeElapsedColumn,
    TimeRemainingColumn,
    MofNCompleteColumn,
)

from pr0loader.config import Settings
from pr0loader.models import PipelineStats
from pr0loader.utils.backoff import fibonacci_backoff
from pr0loader.utils.console import (
    print_header,
    print_stats_table,
    print_success,
    print_info,
    print_warning,
    console,
)

logger = logging.getLogger(__name__)


# =============================================================================
# DATA STRUCTURES
# =============================================================================

@dataclass
class DownloadItem:
    """Minimal download task - just what we need."""
    __slots__ = ('path', 'local_path')
    path: str           # Remote path like "2024/01/01/abc.jpg"
    local_path: Path    # Full local path


@dataclass
class DownloadStats:
    """Statistics for download run."""
    total_in_db: int = 0
    total_on_disk: int = 0
    to_download: int = 0
    downloaded: int = 0
    skipped: int = 0
    failed: int = 0
    bytes_downloaded: int = 0

    # Timing
    load_time: float = 0.0
    download_time: float = 0.0


# =============================================================================
# HELPERS
# =============================================================================

def format_bytes(size: int) -> str:
    """Format bytes to human readable string."""
    for unit in ('B', 'KB', 'MB', 'GB', 'TB'):
        if size < 1024:
            return f"{size:.1f} {unit}"
        size /= 1024
    return f"{size:.1f} PB"


def format_duration(seconds: float) -> str:
    """Format duration to human readable string."""
    if seconds < 60:
        return f"{seconds:.1f}s"
    minutes = seconds / 60
    if minutes < 60:
        return f"{minutes:.1f}m"
    hours = minutes / 60
    return f"{hours:.1f}h"


# =============================================================================
# FAST DATA LOADING
# =============================================================================

def load_db_images(db_path: Path, include_videos: bool = False) -> pd.DataFrame:
    """Load all image paths from database into DataFrame.

    Returns DataFrame with columns: [id, image]
    Uses raw SQL for maximum speed.
    """
    start = time.perf_counter()

    conn = sqlite3.connect(db_path)
    df = pd.read_sql_query(
        "SELECT id, image FROM items WHERE image IS NOT NULL",
        conn
    )
    conn.close()

    # Filter by extension (vectorized)
    allowed = {'.jpg', '.jpeg', '.png', '.gif'}
    if include_videos:
        allowed |= {'.mp4', '.webm'}

    # Extract extension and filter
    df['ext'] = df['image'].str.extract(r'(\.[^.]+)$', expand=False).str.lower()
    df = df[df['ext'].isin(allowed)].drop(columns=['ext'])

    elapsed = time.perf_counter() - start
    logger.debug(f"Loaded {len(df):,} items from DB in {elapsed:.2f}s")

    return df


def scan_filesystem(media_dir: Path, include_videos: bool = False) -> set[str]:
    """Scan filesystem and return set of existing file paths.

    Uses os.scandir for speed (faster than os.walk for flat checks).
    Returns relative paths like "2024/01/01/abc.jpg"
    """
    start = time.perf_counter()

    allowed = {'.jpg', '.jpeg', '.png', '.gif'}
    if include_videos:
        allowed |= {'.mp4', '.webm'}

    existing: set[str] = set()

    def scan_recursive(path: Path, prefix: str = ""):
        """Recursively scan directory."""
        try:
            with os.scandir(path) as entries:
                for entry in entries:
                    if entry.is_file(follow_symlinks=False):
                        ext = os.path.splitext(entry.name)[1].lower()
                        if ext in allowed:
                            rel_path = f"{prefix}{entry.name}" if prefix else entry.name
                            existing.add(rel_path)
                    elif entry.is_dir(follow_symlinks=False):
                        new_prefix = f"{prefix}{entry.name}/" if prefix else f"{entry.name}/"
                        scan_recursive(Path(entry.path), new_prefix)
        except PermissionError:
            pass

    if media_dir.exists():
        scan_recursive(media_dir)

    elapsed = time.perf_counter() - start
    logger.debug(f"Scanned {len(existing):,} files in {elapsed:.2f}s")

    return existing


def compute_download_list(
    df_db: pd.DataFrame,
    existing_files: set[str],
    media_dir: Path,
) -> list[DownloadItem]:
    """Compute list of files to download.

    Fast set-based comparison.
    Returns list of DownloadItem objects.
    """
    start = time.perf_counter()

    # Vectorized membership check
    mask = ~df_db['image'].isin(existing_files)
    to_download = df_db[mask]

    # Convert to download items
    items = [
        DownloadItem(
            path=row['image'],
            local_path=media_dir / row['image'],
        )
        for _, row in to_download.iterrows()
    ]

    elapsed = time.perf_counter() - start
    logger.debug(f"Computed {len(items):,} items to download in {elapsed:.2f}s")

    return items


# =============================================================================
# ASYNC DOWNLOADER
# =============================================================================

class AsyncDownloader:
    """Fast async file downloader with connection pooling."""

    def __init__(
        self,
        base_url: str,
        cookies: dict[str, str],
        max_concurrent: int = 20,
        max_retries: int = 3,
        max_backoff: float = 60.0,
    ):
        self.base_url = base_url.rstrip('/')
        self.cookies = cookies
        self.max_concurrent = max_concurrent
        self.max_retries = max_retries
        self.max_backoff = max_backoff

        # Stats
        self.downloaded = 0
        self.failed = 0
        self.bytes_total = 0

        # Rate limiting state
        self._backoff_attempt = 0

    async def download_all(
        self,
        items: list[DownloadItem],
        progress: Progress,
        task_id,
    ) -> tuple[int, int, int]:
        """Download all items with progress updates.

        Returns: (downloaded, failed, bytes)
        """
        if not items:
            return 0, 0, 0

        # Create connector with connection pooling
        connector = aiohttp.TCPConnector(
            limit=self.max_concurrent,
            limit_per_host=self.max_concurrent,
            ttl_dns_cache=300,
            enable_cleanup_closed=True,
            keepalive_timeout=30,
        )

        # Semaphore for concurrency control
        semaphore = asyncio.Semaphore(self.max_concurrent)

        async with aiohttp.ClientSession(
            connector=connector,
            cookies=self.cookies,
            timeout=aiohttp.ClientTimeout(total=120, connect=10),
        ) as session:

            async def download_one(item: DownloadItem) -> tuple[bool, int]:
                """Download single file. Returns (success, bytes)."""
                async with semaphore:
                    url = f"{self.base_url}/{item.path}"

                    for attempt in range(self.max_retries):
                        try:
                            async with session.get(url) as resp:
                                if resp.status == 200:
                                    # Reset backoff on success
                                    self._backoff_attempt = 0

                                    # Ensure directory exists
                                    item.local_path.parent.mkdir(parents=True, exist_ok=True)

                                    # Stream to file
                                    size = 0
                                    with open(item.local_path, 'wb') as f:
                                        async for chunk in resp.content.iter_chunked(65536):
                                            f.write(chunk)
                                            size += len(chunk)

                                    return True, size

                                elif resp.status == 429:
                                    # Rate limited - backoff
                                    wait = fibonacci_backoff(self._backoff_attempt, self.max_backoff)
                                    self._backoff_attempt += 1
                                    logger.warning(f"Rate limited, waiting {wait:.1f}s")
                                    await asyncio.sleep(wait)
                                    continue

                                elif resp.status == 404:
                                    # File doesn't exist remotely - skip
                                    return False, 0

                                else:
                                    logger.debug(f"HTTP {resp.status} for {item.path}")

                        except asyncio.TimeoutError:
                            logger.debug(f"Timeout for {item.path}, attempt {attempt + 1}")
                        except Exception as e:
                            logger.debug(f"Error for {item.path}: {e}, attempt {attempt + 1}")

                        # Exponential backoff between retries
                        if attempt < self.max_retries - 1:
                            await asyncio.sleep(2 ** min(attempt, 3))

                    # All retries failed
                    logger.warning(f"Failed after {self.max_retries} attempts: {item.path}")
                    return False, 0

            # Process all items
            tasks = []
            for item in items:
                task = asyncio.create_task(download_one(item))
                tasks.append(task)

            # Gather with progress updates
            downloaded = 0
            failed = 0
            bytes_total = 0

            for coro in asyncio.as_completed(tasks):
                success, size = await coro
                if success:
                    downloaded += 1
                    bytes_total += size
                else:
                    failed += 1

                # Update progress
                progress.update(task_id, advance=1)

        return downloaded, failed, bytes_total


# =============================================================================
# MAIN PIPELINE
# =============================================================================

class DownloadPipeline:
    """High-performance download pipeline.

    Fast as fuck boi! ðŸš€

    Optimizations:
    - Pandas for bulk data loading (no ORM overhead)
    - Set operations for file comparison (O(n) not O(nÂ²))
    - Async I/O for concurrent downloads
    - Connection pooling with keep-alive
    - Minimal object creation
    - No blocking in event loop
    """

    def __init__(self, settings: Settings):
        self.settings = settings
        self.stats = DownloadStats()

        # Tunable parameters
        self.max_concurrent = settings.max_parallel_requests
        self.max_retries = settings.max_retries
        self.max_backoff = settings.max_backoff_seconds

    def _create_progress(self) -> Progress:
        """Create progress bar."""
        return Progress(
            SpinnerColumn(),
            TextColumn("[bold blue]{task.description}"),
            BarColumn(bar_width=40),
            MofNCompleteColumn(),
            TextColumn("â€¢"),
            TimeElapsedColumn(),
            TextColumn("â€¢"),
            TimeRemainingColumn(),
            console=console,
            transient=False,
        )

    def run(
        self,
        include_videos: bool = False,
        verify_existing: bool = False,
    ) -> PipelineStats:
        """Run the download pipeline.

        Args:
            include_videos: Download videos too (default: images only)
            verify_existing: Re-download if size differs (default: skip existing)
        """
        print_header(
            "ðŸ“ Download Media",
            "Fast as fuck downloader ðŸš€"
        )

        media_dir = self.settings.filesystem_prefix
        media_dir.mkdir(parents=True, exist_ok=True)

        print_info(f"Media directory: {media_dir}")
        print_info(f"Mode: {'Images + Videos' if include_videos else 'Images only'}")
        print_info(f"Workers: {self.max_concurrent}")

        total_start = time.perf_counter()

        # =====================================================================
        # PHASE 1: LOAD DATA
        # =====================================================================
        print_info("Phase 1: Loading data...")
        load_start = time.perf_counter()

        with ThreadPoolExecutor(max_workers=2) as executor:
            # Load DB and scan FS in parallel
            db_future = executor.submit(
                load_db_images,
                self.settings.db_path,
                include_videos,
            )
            fs_future = executor.submit(
                scan_filesystem,
                media_dir,
                include_videos,
            )

            df_db = db_future.result()
            existing_files = fs_future.result()

        self.stats.total_in_db = len(df_db)
        self.stats.total_on_disk = len(existing_files)

        print_info(f"  Database: {self.stats.total_in_db:,} items")
        print_info(f"  On disk:  {self.stats.total_on_disk:,} files")

        # Compute what to download
        items = compute_download_list(df_db, existing_files, media_dir)
        self.stats.to_download = len(items)
        self.stats.skipped = self.stats.total_in_db - self.stats.to_download

        self.stats.load_time = time.perf_counter() - load_start
        print_info(f"  To download: {self.stats.to_download:,} files")
        print_info(f"  Load time: {format_duration(self.stats.load_time)}")

        if not items:
            print_success("Nothing to download - all files exist!")
            return self._to_pipeline_stats()

        # =====================================================================
        # PHASE 2: DOWNLOAD
        # =====================================================================
        print_info(f"Phase 2: Downloading {len(items):,} files...")
        download_start = time.perf_counter()

        # Create downloader
        downloader = AsyncDownloader(
            base_url=self.settings.media_base_url,
            cookies={
                'me': self.settings.me,
                'pp': self.settings.pp,
            },
            max_concurrent=self.max_concurrent,
            max_retries=self.max_retries,
            max_backoff=self.max_backoff,
        )

        # Run downloads with progress
        progress = self._create_progress()

        with progress:
            task_id = progress.add_task(
                "[cyan]Downloading...",
                total=len(items),
            )

            downloaded, failed, bytes_total = asyncio.run(
                downloader.download_all(items, progress, task_id)
            )

        self.stats.downloaded = downloaded
        self.stats.failed = failed
        self.stats.bytes_downloaded = bytes_total
        self.stats.download_time = time.perf_counter() - download_start

        # =====================================================================
        # DONE
        # =====================================================================
        total_time = time.perf_counter() - total_start

        # Calculate rates
        if self.stats.download_time > 0:
            files_per_sec = self.stats.downloaded / self.stats.download_time
            bytes_per_sec = self.stats.bytes_downloaded / self.stats.download_time
        else:
            files_per_sec = 0
            bytes_per_sec = 0

        print_stats_table("Download Results", {
            "Downloaded": f"{self.stats.downloaded:,} files",
            "Skipped": f"{self.stats.skipped:,} files",
            "Failed": f"{self.stats.failed:,} files",
            "Data": format_bytes(self.stats.bytes_downloaded),
            "Speed": f"{files_per_sec:.1f} files/s ({format_bytes(int(bytes_per_sec))}/s)",
            "Load time": format_duration(self.stats.load_time),
            "Download time": format_duration(self.stats.download_time),
            "Total time": format_duration(total_time),
        })

        print_success("Download complete! ðŸŽ‰")

        return self._to_pipeline_stats()

    def _to_pipeline_stats(self) -> PipelineStats:
        """Convert to PipelineStats for compatibility."""
        stats = PipelineStats()
        stats.files_downloaded = self.stats.downloaded
        stats.files_skipped = self.stats.skipped
        stats.items_failed = self.stats.failed
        stats.bytes_downloaded = self.stats.bytes_downloaded
        return stats

